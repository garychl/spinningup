{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import itertools \n",
    "import matplotlib \n",
    "import matplotlib.style \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys \n",
    "\n",
    "from collections import defaultdict \n",
    "\n",
    "matplotlib.style.use('ggplot') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "ttl_episodes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garychan/miniconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions):\n",
    "    def policyFunction(state):\n",
    "        Action_probabilities = np.ones(num_actions, dtype=float) * epsilon / num_actions\n",
    "        best_action = np.argmax(Q[state])\n",
    "        print(f'best action: {best_action}')\n",
    "        print(f'prob of best action before update: {Action_probabilities[best_action]}')\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
    "        print(f'prob of best action after update: {Action_probabilities[best_action]}')        \n",
    "        return Action_probabilities\n",
    "    return policyFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor=1.0, alpha=0.6, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    episode_lengths = np.zeros(num_episodes)\n",
    "    episode_rewords = np.zeros(num_episodes)\n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_sapce.n)\n",
    "    for ith_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        for t in itertools.count():\n",
    "            action_probabilities = policy(state)\n",
    "            action = np.random.choice(\n",
    "            np.arange(len(action_probabilities)), \n",
    "            p = action_probabilities)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spinningup)",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
